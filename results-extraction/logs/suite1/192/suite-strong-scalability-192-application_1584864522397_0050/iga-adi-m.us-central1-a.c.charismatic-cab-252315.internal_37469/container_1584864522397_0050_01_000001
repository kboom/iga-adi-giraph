Container: container_1584864522397_0050_01_000001 on iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal_37469
LogAggregationType: AGGREGATED
====================================================================================================================
LogType:gam-stderr.log
LogLastModifiedTime:Sun Mar 22 12:38:33 +0000 2020
LogLength:518
LogContents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/nm-local-dir/usercache/kbhit/appcache/application_1584864522397_0050/filecache/11/solver-1.0-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]

End of LogType:gam-stderr.log
*******************************************************************************

Container: container_1584864522397_0050_01_000001 on iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal_37469
LogAggregationType: AGGREGATED
====================================================================================================================
LogType:gam-stdout.log
LogLastModifiedTime:Sun Mar 22 12:38:33 +0000 2020
LogLength:267660
LogContents:
INFO  [GiraphApplicationMaster] Starting GitaphAM 
INFO  [GiraphApplicationMaster] GiraphAM  for ContainerId container_1584864522397_0050_01_000001 ApplicationAttemptId appattempt_1584864522397_0050_000001
INFO  [GiraphApplicationMaster] Yarn client user: kbhit
INFO  [GiraphApplicationMaster] Requested container ask: Capability[<memory:113050, vCores:0>]Priority[10]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]
INFO  [GiraphApplicationMaster] Requested container ask: Capability[<memory:113050, vCores:0>]Priority[10]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]
INFO  [GiraphApplicationMaster] Wait to finish ..
INFO  [GiraphApplicationMaster$RMCallbackHandler] Got response from RM for container ask, allocatedCnt=1
INFO  [GiraphApplicationMaster$RMCallbackHandler] Total allocated # of container so far : 1 allocated out of 2 required.
INFO  [GiraphApplicationMaster] Launching command on a new container., containerId=container_1584864522397_0050_01_000002, containerNode=iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal:37469, containerNodeURI=iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal:8042, containerResourceMemory=113664
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] START OF GIRAPH CONFIGURATION
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobtracker.address=local
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.file-formats=TFile
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.resource.check.interval=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.posix.attr.uid.name=uidNumber
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.client.thread-count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.invalidate.limit=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
      $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
      $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
      /usr/local/share/google/dataproc/lib/*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.admin.acl=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.emit-timeline-data=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.rpc-address=0.0.0.0:8485
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxWorkers=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/system/rmstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.select-input-streams.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connection.maxidletime=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.process-kill-wait.ms=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.memoryObserver.minMsBetweenFullGcs=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.use-rpc=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.map.index.interval=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.https-address=0.0.0.0:9871
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.max-no-move-interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.multipart.uploads.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.reduces=0-2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.util.hash.type=murmur
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexInputFormatClass=edu.agh.iga.adi.giraph.direction.io.InMemoryStepInputFormat
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.min=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.path.style.access=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.min-block-size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] net.topology.script.number.args=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.initial.type=RADIAL
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.windows-container.memory-limit.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.input.fileinputformat.split.minsize=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.max.attempts=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.speculative=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.node-labels.resync-interval-ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.interval=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.maps=285
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.ubertask.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.retain-seconds=604800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.use.datanode.hostname=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.getBlocks.size=2147483648
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.am.max-attempts=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.data.dir=/hadoop/dfs/data
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.parallelcopies=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] adl.feature.ownerandgroup.enableupn=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.buffer.dir=${hadoop.tmp.dir}/s3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.finish-when-all-reducers-done=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.retry.ceiling.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.count-threshold=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.edgeInputFormatClass=edu.agh.iga.adi.giraph.direction.io.IgaEdgeInputFormat
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.metrics.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.data.dir.perm=700
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.movedWinWidth=5400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.xattrs.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.bp-ready.timeout=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.transfer.socket.send.buffer.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.ha.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client.job.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.table.capacity.write=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.recovery.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.container.log.backups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.interval-ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.cache.directives.num.responses=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.node-labels.provider.fetch-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.max.total.tasks=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.port=13562
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.maxattempts=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.resource-tracker.client.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.considerLoad=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.cross-origin.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.bind-host=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.delete.thread-count=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.pending.timeout-sec=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.mutation.acl-policy.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.proxy-user-privileges.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.speculative-cap-total-tasks=0.01
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.replication=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.slowtaskthreshold=1.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.cleaner.initial-delay-mins=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.slow.io.warning.threshold.ms=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.default-container-executor.log-dirs.permissions=710
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.skip.start.attempts=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.dns.log-slow-lookups.threshold.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.reject-unresolved-dn-topology-mapping=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size=10485760
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.admin.address=0.0.0.0:8047
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socket-timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numOutputThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.max-streams-hard-limit=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.num.users=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.mount=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.classloader=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation-enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanager.minimum.version=NONE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.size=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.compress.type=RECORD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.hdfs.configuration.version=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.done-dir=/tmp/entity-file-history/done/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-app-masters=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log.retain-seconds=10800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.local-cache.max-files-per-directory=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.retry.interval=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.new-active.rpc-timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.hostname.verifier=DEFAULT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.audit.log.token.tracking.id=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.bq.project.id=charismatic-cab-252315
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.sleep.base.millis=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.permissions.superusergroup=hadoop
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.retry.times=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socket.send.buffer.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.fileoutputcommitter.task.cleanup.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.getBlocks.min-block-size=10485760
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.initialDelay=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.edgeValueClass=edu.agh.iga.adi.giraph.direction.io.data.IgaOperationWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.maximum-allocation-mb=294912
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.io.sort.factor=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.with-user-dir=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.sleep.max.millis=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.webapp.https.address=0.0.0.0:8091
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.sleepTimeSeconds=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log.deletion-threads-count=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.rpc-timeout.ms=45000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.file.impl=org.apache.hadoop.fs.LocalFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.host=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxOpenRequestsPerWorker=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.client.user=kbhit
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.adl.oauth2.access.token.provider.type=ClientCredential
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.active-dir=/tmp/entity-file-history/active
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.cores=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.background.sleep=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-blocks-per-file=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation.key.update-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.https.address=0.0.0.0:8044
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.keytab.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lifeline.handler.ratio=0.10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.compression.codec.bzip2.library=system-native
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.skip.maxrecords=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.ping.interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.zkfc.port=8019
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.window.base=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.loadedjobs.cache.size=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.credentials.binary=/hadoop/yarn/nm-local-dir/usercache/kbhit/appcache/application_1584864522397_0050/container_1584864522397_0050_01_000001/container_tokens
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.storage.policy.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.output.filter=FAILED
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.count-logical-processors-as-cores=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.system-reserved-memory-mb=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.best-effort=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.block-pinning.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.retry-after-no-speculate=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.seqfile.local.dir=${hadoop.tmp.dir}/io/local
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ui-names=tez
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.encrypt.data.transfer.cipher.key.bitlength=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.log.level=INFO
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.sync.behind.writes=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.dispatcherThreads=200
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.stale.datanode.interval=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.io.sort.mb=256
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-state-store.parent-path=/rmstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.clientSendBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.client.resolve.remote.symlinks=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.min-queue-length=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.enabled.protocols=TLSv1,TLSv1.1,TLSv1.2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.cpu.vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.failover-retries=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.address=iga-adi-m:10020
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.name.dir=/hadoop/dfs/name
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.leveldb-store.compaction-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.local-fs.write-limit.bytes=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.configuration.file-system-based-store=/yarn/conf
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.access.token.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD,TRACE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.retry-after-speculate=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.fileio.profiling.sampling.percentage=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.address=0.0.0.0:9866
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-aggregation.policy.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ls.limit=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.max.retries=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container.stderr.tail.bytes=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.timeout=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ui-on-disk-path.tez=/usr/lib/tez/tez-ui-0.9.2.war
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.reservation-system.planfollower.time-step=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.custom-header=X-XSRF-HEADER
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.automatic-failover.embedded=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.handler.count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.channelsPerServer=8
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.msgRequestWarningThreshold=1.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-container-debug-info.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.map.params=${mapreduce.task.profile.params}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.logThreadLayout=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.block-placement-policy.default.prefer-local-node=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.hosts=/etc/hadoop/conf/nodes_include
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.resource.checked.volumes.minimum=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.retry.max.attempts=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.keytab=/etc/krb5.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.cluster.max-application-priority=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation-status.time-out.ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.state-store.class=org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.max-cached-nodemanagers-proxies=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.trash.checkpoint.interval=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.http-address=0.0.0.0:8480
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nm.liveness-monitor.expiry-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useSuperstepCounters=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.merge.percent=0.66
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.retrycache.heap.percent=0.03f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.timeout=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-removal-untracked.remove-on-refresh=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.store.class=file
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max-corrupt-file-blocks-returned=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.local-dirs=/hadoop/yarn/nm-local-dir
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.block-move.timeout=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.replication=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.am.max-attempts=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.internal-timers-ttl-secs=420
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.pcores-vcores-multiplier=1.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useInputSplitLocality=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.kerberos.min.seconds.before.relogin=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.compress=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.attempt.diagnostics.limit.kc=64
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ui-web-path.tez=/tez-ui
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.steps=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.ssl=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.check.period=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.defaultFS=hdfs://iga-adi-m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max-num-blocks-to-log=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.attr.group.name=cn
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.service.handler.count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.pipeline.cache-max-size=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.sort.spill.percent=0.80
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.file-controller.TFile.class=org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.http-address=0.0.0.0:9870
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.metadatastore.impl=org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.negative-cache.secs=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.server.conf=ssl-server.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.nodemanager-client-async.thread-pool-max-size=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.yarn.nodemanager.resource.memory-mb=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.admin.address=0.0.0.0:10033
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.metadata.cache.type=FILESYSTEM_BACKED
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.startup.delay.block.deletion.sec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.health-checker.interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.keytab=/etc/krb5.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.task.container.log.backups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.heartbeat.interval=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.zookeeper.session-timeout.ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] container.java.opts=-XX:+UseNUMA -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:NewSize=9G -XX:MaxNewSize=9G -server -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -XX:OnOutOfMemoryError='free -m'
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.zkList=iga-adi-m:2181
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-aggregation.compression-type=none
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-retry-minimum-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.minimum-allowed-tasks=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.cache.revocation.timeout.ms=900000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.combine.progress.records=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.supervised=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.interceptor-class.pipeline=org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.low-latency=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.address=${yarn.nodemanager.hostname}:0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reduces=95
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.queue-placement-rules=user-group
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.session.timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] tfile.io.chunk.size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.sleep-after-disconnect.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.http-cross-origin.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.user.agent.prefix=unknown
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.content-summary.sleep-microsec=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.directoryscan.threads=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.directoryscan.interval=21600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.token.validity=36000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.cleaner.interval-ms=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.datanode.registration.ip-hostname-check=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.backup.http-address=0.0.0.0:50105
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.opportunistic-containers-max-queue-length=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.reservation-system.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.crypto.buffer.size=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.read.timeout=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.transfer.mode=BLOCK_TRANSFER_MODE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.ifile.readahead.bytes=4194304
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.secure=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.safemode.min.datanodes=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.wasb.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.snapshot.skip.capture.accesstime-only-change=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.http-authentication.type=simple
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.socket.read-timeout=60s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.dispatcher.drain-events.timeout=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.avoid.write.stale.datanode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.retain-seconds=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.complete.cancel.delegation.tokens=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fail-fast=${yarn.fail-fast}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multiobjectdelete.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.connection-keep-alive.timeout=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.minimum-allocation-vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.max-retries=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.retry-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.exports.allowed.hosts=* rw
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.max.threads=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.cache.size=256
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.file.buffer.size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.bind-host=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.max.block.acquire.failures=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.zkfc.nn.http.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction=0.6
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-metrics.unregister-delay-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.interval-ms.get-last-block-length=4000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.txns=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.retry.interval=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.edits.dir=/tmp/hadoop/dfs/journalnode/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-metrics.period-ms=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.connect.timeout=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.uri=file:///hadoop/yarn/system/rmstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.drain-entities.timeout.ms=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.adl.impl=org.apache.hadoop.fs.adl.Adl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.connection.timeout.ms=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.openfiles.num.responses=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.cachereport.intervalMsec=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.fd-flush-interval-secs=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.tail-edits.rolledits.timeout=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.container.log.limit.kb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resourcemanager.minimum.version=NONE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.ubertask.maxreduces=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanager-connect-retries=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.secure.mode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.retry.policy.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.idlethreshold=4000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.group.hierarchy.levels=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.logaggregation.threadpool-size-max=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.quota.by.storage.type.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.collector-service.address=${yarn.nodemanager.hostname}:8048
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.client-server.address=0.0.0.0:8045
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.connection.retries.on.timeouts=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxNumberOfUnsentRequests=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.work.multiplier.per.iteration=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.simple.anonymous.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.nodemanager-connect.retry-interval-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.times.get-last-block-length=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.app-collector.linger-period.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.authentication=simple
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.files.preserve.failedtasks=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds=259200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.streams.cache.size=256
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.finalize-segment.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.replication=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.skip.proc-count.auto-incr=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.client.thread-count=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.cache-ttl.secs=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.joblist.cache.size=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-xattrs-per-inode=32
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.work-preserving-recovery.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer.timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.wtmax=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.worker.observers=edu.agh.iga.adi.giraph.direction.performance.MemoryLogger
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.purge=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.ui-actions.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.establish.timeout=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] rpc.engine.org.apache.hadoop.yarn.api.ApplicationHistoryProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.rm.system-metrics-publisher.emit-container-events=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.invalidate.work.pct.per.iteration=0.32f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.purge.age=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.client.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.auto-update.containers=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.leveldb-store.path=${hadoop.tmp.dir}/yarn/system/confstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.maximum.data.length=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] tfile.fs.input.buffer.size=262144
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.type=simple
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.encryption.zones.num.responses=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.async.message.store.threads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.cpu.vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.decommission.interval=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-delegation-token-node.split-index=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.webhdfs.impl=org.apache.hadoop.fs.WebHdfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.pipeline.ecn=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.user.home.dir.prefix=/user
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.workaround.non.threadsafe.getpwuid=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.pmem-check-enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.inotify.max.events.per.rpc=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.maps=0-2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.ssl.file.buffer.size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.transfer.socket.recv.buffer.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.registry.class=org.apache.hadoop.registry.client.impl.FSRegistryOperationsService
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.command-opts=-Xmx2457m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.timeline-client.number-of-async-entities-to-merge=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.amlauncher.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.failover.sleep.max.millis=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.nm.uploader.replication.factor=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.lock.suppress.warning.interval=10s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.moverThreads=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.root=/registry
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.max.attempts=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.memoryObserver.freeMemoryFractionForGc=0.1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.fd-retain-secs=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.caller.context.max.size=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.max-age=1800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.remote-app-log-dir-suffix=logs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.restart.replica.expiration=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.dir.minimum=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.mountd.port=4242
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.merge.inmem.threshold=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.netty.low.watermark=32768
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lifeline.rpc-address=iga-adi-m:8050
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.num.checkpoints.retained=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.queuename=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.max-age-ms=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.authorization=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.aux-services.spark_shuffle.class=org.apache.spark.network.yarn.YarnShuffleService
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.client.thread-count=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.uploader.server.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.serverSendBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fslock.fair=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.split.threshold=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.scanner.volume.bytes.per.second=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.balance.bandwidthPerSec=10m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.buffer.size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.rpc-timeout.ms=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.default.chunk.view.size=32768
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.datestring.cache.size=200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.handler.count=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.acl.reservation-enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer.bandwidthPerSec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.loadedjob.tasks.max=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.clientReceiveBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.local.sas.key.mode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.threads.max=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.handler-thread-count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.server.listen.queue.size=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.connect.max-wait.ms=900000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.detect-hardware-capabilities=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.min-replication=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.max.split.locations=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyAutoRead=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blocksize=134217728
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.is.minicluster=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.connection-keep-alive.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.threads.keepalivetime=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.shell.command.timeout=0s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-validator=basic
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.cli-check.rpc-timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.zookeeper.acl=world:anyone:rwcda
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.write.stale.datanode.ratio=0.5f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.encrypt.data.transfer=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.input.lineinputformat.linespermap=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.fetch.thread-count=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyClientThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexIdClass=org.apache.hadoop.io.IntWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.skip.checksum=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.service.shutdown.timeout=30s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.ssl.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.log.level=INFO
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.SplitMasterWorker=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation-token.max-conf-size-bytes=12800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.rm.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.quota.init-threads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.opportunistic-container-allocation.nodes-used=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.cache.limit.max-resources-mb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.metrics.logger.period.seconds=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.acl.provider.permission.pattern=^(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?)*$
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.use.datanode.hostname=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.address=0.0.0.0:0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lock.detailed-metrics.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.rpc-address=iga-adi-m:8020
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.application.name.suffix=-dataproc
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.libjars=solver-1.0-SNAPSHOT.jar
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.reformat.disabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.max-packets-in-flight=80
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.threshold=2147483647
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.memory.limit.percent=0.25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.https.server.keystore.resource=ssl-server.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.include-port-in-node-name=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] map.sort.class=org.apache.hadoop.util.QuickSort
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.dns.interface=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitForPerWorkerRequests=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.retry-delay.max.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-max-znode-size.bytes=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.progressmonitor.pollinterval=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.shuffle.log.limit.kb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.max.locked.memory=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.retrycache.expirytime.millis=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.scan.period.hours=504
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.fencing.ssh.connect-timeout=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.move.interval-ms=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.best-effort=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.disk.check.min.gap=15m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hbase-schema.prefix=prod.
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-component-length=255
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.ipc.address=0.0.0.0:9867
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.path.based.cache.retry.interval.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.misreplication.processing.limit=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.fast.upload.active.blocks=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.opportunistic-container-allocation.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.tail-edits.period=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.generic-application-history.max-applications=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.resendTimedOutRequests=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.jaas.context=Client
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.hostname=iga-adi-m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.policy.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.shell.safely.delete.limit.num.files=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.dir=${dfs.namenode.name.dir}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.failover-retries-on-socket-timeouts=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.decommission.max.concurrent.tracked.nodes=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.server.log.slow.rpc=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.store.leveldb.path=${hadoop.tmp.dir}/mapred/history/recoverystore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.support.append=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.windows-container.cpu-limit.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.socket.reuse.keepalive=4000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.vmem-pmem-ratio=2.1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.period=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.node-labels.provider.fetch-timeout-ms=1200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.java.opts=-Xmx2457m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numInputThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hbase.coprocessor.jar.hdfs.location=/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.automatic-failover.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitTimeBetweenConnectionRetriesMs=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.webapp.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.use.dfs.network.topology=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.child.java.opts=-Xmx200m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.https.need-auth=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.write-lock-reporting-threshold-ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.block.size=32M
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.host.port=21
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.avoid.read.stale.datanode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.retry.attempts=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.allowed-runtimes=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lease-recheck-interval-ms=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.locateFollowingBlock.initial.delay.ms=400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.cluster.acls.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.retry.max.attempts=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.aux-services=mapreduce_shuffle,spark_shuffle
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.ubertask.maxmaps=9
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.hosts.exclude=/etc/hadoop/conf/nodes_exclude
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.default-container-network=host
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-manager.thread-count=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socketcache.expiryMsec=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.app-submission.cross-platform=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.metadatastore.authoritative=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reducer.preempt.delay.sec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.path.based.cache.block.map.allocation.percent=0.25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexRequestSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.rerun-if-node-unusable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.use.ipc.callq=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.cache-store-class=org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.cache.revocation.polling.ms=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.markreset.buffer.percent=0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lazypersist.file.scrub.interval.sec=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.retry.interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.keytab=/etc/krb5.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.project.id=charismatic-cab-252315
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.dump.dir=/tmp/.hdfs-nfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.movedWinWidth=5400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] datanode.https.port=50475
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ttl-enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.cached-dfsused.check.interval.ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.delete.debug-delay-sec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.skip.maxgroups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.trash.interval=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.moverThreads=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxNumberOfSupersteps=2147483647
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource-monitor.interval-ms=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] seq.io.sort.factor=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] seq.io.sort.mb=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.instrumentation.requires.admin=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.rtmax=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.serverReceiveBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.backup.address=0.0.0.0:50100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max-lock-hold-to-release-lease-ms=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.readahead.bytes=4194304
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.random.order=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.minWorkers=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.cleaner.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.graceful-fence.connection.retries=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] httpfs.buffer.size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.safemode.threshold-pct=0.999f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-monitor.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.pure.yarn.job=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.java.secure.random.algorithm=SHA1PRNG
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.dns.nameserver=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.submit.file.replication=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.fail-fast=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.cache.timeout.ms=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.file.close.num-committed-allowed=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.skip.checksum.errors=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hostname=iga-adi-m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.load-comparator=QUEUE_LENGTH
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.acl.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.fast.upload=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.partitionClass=org.apache.giraph.partition.SimplePartition
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation-token-renewer.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.swebhdfs.impl=org.apache.hadoop.fs.SWebHdfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.application-timeouts.monitor.interval-ms=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.common.configuration.version=0.23.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.client.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.edgeRequestSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.drop.cache.behind.reads=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.outputdir=hdfs://iga-adi-m/user/kbhit/1584880683
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.use.legacy.blockreader=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.max-completed-applications=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.xframe.value=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.cleaner.period-mins=1440
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.registry.base-dir=yarnfederation/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.max.retry.interval=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.always-scan-user-dir=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.fs-store.retry-policy-spec=2000, 500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.acl-view-job= 
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.job.task.listener.thread-count=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edit.log.autoroll.check.interval.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.resource.cpu-vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.skip.proc-count.auto-incr=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.attr.member=member
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.client.conf=ssl-client.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.start-segment.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.root-dir=/sharedcache
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.https-address=0.0.0.0:8481
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.background.reload=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.objects=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.max.transfer.threads=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.credential.clear-text-fallback=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.access.key.update.interval=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.memory.mb=3072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.hdfs-blocks-metadata.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer.chunksize=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.jhist.format=json
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.https.keystore.resource=ssl-client.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.connect.retry-interval.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.minimum-allocation-mb=1024
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.messageStoreFactoryClass=edu.agh.iga.adi.giraph.direction.io.InMemoryObjectMessageStoreFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.cleaner.resource-sleep-ms=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] net.topology.impl=org.apache.hadoop.net.NetworkTopology
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.seqfile.compress.blocksize=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-executor.os.sched.priority.adjustment=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.running.reduce.limit=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.hedged.read.threadpool.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.heartbeat.recheck-interval=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.task.overhead.percent=0.2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.safemode.extension=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.vmem-check-enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.failover.sleep.base.millis=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.key.update-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.rpc.protection=authentication
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.permissions.umask-mode=022
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useMessageSizeEncoding=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.staticuser.user=dr.who
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.maximum=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.methods-allowed=GET,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.paging.maximum=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation.token.renew-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.maximum.response.length=134217728
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.shell.missing.defaultFs.warning=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-localizer.java.opts=-Xmx256m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.saskey.usecontainersaskeyforallaccess=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.dir=file:///hadoop/dfs/namesecondary
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.browser-useragents-regex=^Mozilla.*,^Opera.*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.windows.minutes=1,5,25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.use.legacy.blockreader.local=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.maxtaskfailures.per.tracker=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.max.connections=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.outEdgesClass=edu.agh.iga.adi.giraph.direction.io.IgaArrayEdges
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.application-client-protocol.poll-interval-ms=200
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.inputOutEdgesClass=edu.agh.iga.adi.giraph.direction.io.IgaArrayEdges
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.cache.pools.num.responses=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.server.port=2049
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.checksum.type=CRC32C
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.readahead.range=64K
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.short.circuit.replica.stale.threshold.ms=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.zookeeper.parent-znode=/hadoop-ha
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.admin.thread-count=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.cpu-vcores=96
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.http.policy=HTTP_ONLY
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.attempts.maximum=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.lazywriter.interval.sec=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.retain-check-interval-seconds=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.multipart.copy.block.size=5368709120
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-ip-cache.expiry-interval-secs=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.fd-clean-interval-secs=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.wasbs.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.num-retries=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.split.metainfo.maxsize=10000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.random.device.file.path=/dev/urandom
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.maxRetries=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.nodemanager-connect.max-wait-ms=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client-am.ipc.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-diagnostics-maximum-size=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.metadata.cache.directory=/hadoop_gcs_connector_metadata_cache
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useNettyPooledAllocator=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.replication.max=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.https.address=0.0.0.0:9865
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.standby.checkpoints=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.kill.max=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.memory=92
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.committer.setup.cleanup.needed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.domain.socket.data.traffic=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.cache.target-size-mb=10240
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.admin.client.thread-count=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hash.partitionBalanceAlgorithm=static
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.connection.timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.queue-limit-stdev=1.0f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numMasterZkInputSplitThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-appid-node.split-index=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.tmp.dir=/hadoop/tmp
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.failover.sleep.base.millis=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.configuration-type=centralized
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.http.internal-proxy.port=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ttl-ms=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.exit.timeout.check-interval-ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.speculative=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.block.size=134217728
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.recovery.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.counters.max=120
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.name.cache.threshold=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.full.block.report.leases=6
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexValueClass=edu.agh.iga.adi.giraph.direction.io.data.IgaElementWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.workerContextClass=edu.agh.iga.adi.giraph.direction.IgaWorkerContext
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.doOutputDuringComputation=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.extra.edits.segments.retained=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.webapp.ui2.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.log.level=INFO
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.file-block-storage-locations.timeout.millis=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.fuse.timer.period=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.timeout-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.check-interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.hedged.read.threshold.millis=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.compress=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.native.lib.available=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.in-memory.staleness-period-mins=10080
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.count-limit=2048
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.providers.combined=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.running.map.limit=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.input.buffer.percent=0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.computationClass=edu.agh.iga.adi.giraph.direction.computation.initialisation.InitialComputation
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.slow.io.warning.threshold.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.size=100M
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.job.committer.commit-window=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyServerThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.new-epoch.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.working.dir=/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.asynclogging=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.bq.output.buffer.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.reader.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.incremental.intervalMsec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.network.counts.cache.max.size=2147483647
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.minPercentResponded=100.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.writer.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.ifile.readahead=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.replication=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.get-journal-state.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitForOtherWorkersMsec=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.summary-store=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socketcache.capacity=16
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.table.create=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.policy.spec=10000,6,60000,10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.socket.recv.buffer=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.fsdatasetcache.max.threads.per.volume=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.in-memory.initial-delay-mins=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.address=iga-adi-m:19888
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.adl.impl=org.apache.hadoop.fs.adl.AdlFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.userlog.limit.kb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.ssl.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.rmadmin.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.webapp.address=0.0.0.0:8788
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.fuse.connection.timeout=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.retry.policy.spec=10000,6,60000,10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.rm.container-allocation.expiry-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.server.max.connections=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.resource.mb=3072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.secs=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.peer.stats.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.shuffle.output.buffer.size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.replication=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numComputeThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.transfer.buffer.size=131072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.audit.log.async=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.directory.search.timeout=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.disk.check.timeout=10m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.checksum.combine.mode=MD5MD5CRC
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.maximum-allocation-vcores=32000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.sleep-delay-before-sigkill.ms=250
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.acl-modify-job= 
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.automatic.close=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.sas.expiry.period=90d
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjm.operations.timeout=60s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.stale.datanode.minimum.interval=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.size=192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.background.reload.threads=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.input.fileinputformat.list-status.num-threads=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.posix.attr.gid.name=gidNumber
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.du.reserved.pct=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.acls.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.short.circuit.replica.stale.threshold.ms=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.block.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.resource.du.reserved=104857600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.server-defaults.validity.period.ms=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.listen.queue.size=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.libjars.wildcard=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.compaction-interval-secs=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.noeditlogchannelflush=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.input.buffer.percent=0.70
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.http.policy=HTTP_ONLY
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.maxattempts=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.bq.gcs.bucket=dataproc-43b93b37-4b7e-4692-b919-138ab6d725e7-us-central1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.audit.loggers=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.serializations=org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.warn.after.ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.count-reset-time-period-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.snapshot.capture.openfiles=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.fs-store.impl.class=org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.queued-edits.limit.mb=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.policy=HTTP_ONLY
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.max-size-to-move=10737418240
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.sync.behind.writes.in.background=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.acl=world:anyone:rwcda
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexOutputFormatThreadSafe=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.file-block-storage-locations.num-threads=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container.stderr.pattern={*stderr*,*STDERR*}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.cluster.local.dir=/hadoop/mapred/local
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.kerberos.kinit.command=kinit
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.secondary.https-address=0.0.0.0:9869
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.metrics.logger.period.seconds=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.access.token.lifetime=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.token.max-lifetime=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.drop.cache.behind.writes=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.considerLoad.factor=2.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.num.extra.edits.retained=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.implicit.dir.infer.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.max.retries.on.timeouts=45
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.viewfs.rename.strategy=SAME_MOUNTPOINT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.client.resolve.topology.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.proxyuser.hive.hosts=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-labels.provider.fetch-interval-ms=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-metrics.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.block.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.fast.upload.buffer=disk
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.connect-retry-interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edekcacheloader.initial.delay.ms=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.tasktracker.map.tasks.maximum=96
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.datanode-restart.timeout=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.mapfile.bloom.size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.authentication.retry-count=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.data.connection.mode=ACTIVE_LOCAL_DATA_CONNECTION_MODE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.preferIP=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.max.map=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.shuffle.log.backups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexOutputFormatClass=edu.agh.iga.adi.giraph.direction.io.StepVertexOutputFormat
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxMasterSuperstepWaitMsecs=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.kerberos.principal.pattern=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.monitor.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.socket.connect-timeout=60s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.max-streams=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.allow.insecure.ports=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.nm.uploader.thread-count=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.graphPartitionerFactoryClass=edu.agh.iga.adi.giraph.direction.IgaPartitionerFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client.job.retry-interval=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.store.max-logs=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.authorization=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.version=1.0f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.am.liveness-monitor.expiry-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.har.impl.disable.cache=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reduce.slowstart.completedmaps=0.95
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.upgrade.domain.factor=${dfs.replication}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.minicluster.fixed.ports=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.application.classpath=$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
      $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
      /usr/local/share/google/dataproc/lib/*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation.token.max-lifetime=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.automatic-failover.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.java.opts=-Xmx2457m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.socket.write.timeout=480000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.accesstime.precision=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.mapfile.bloom.error.rate=0.005
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/timeline
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.proxyuser.hive.groups=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.support.allow.format=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.zk-store.parent-path=/confstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.content-summary.limit=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.writer.flush-interval-seconds=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodes.exclude-path=/etc/hadoop/conf/nodes_exclude
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.outliers.report.interval=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.shuffle.log.separate=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.user.group.static.mapping.overrides=dr.who=;
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.address=0.0.0.0:8049
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.webapp.xfs-filter.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.cached.conn.retry=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.key.provider.cache.expiry=864000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.path.based.cache.refresh.interval.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.collector-service.thread-count=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodes.include-path=/etc/hadoop/conf/nodes_include
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-directory-items=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.log-roll.period=120
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.capabilities=CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.distributed-scheduling.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.fallback-to-simple-auth-allowed=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.fixed.ports=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.remote-app-log-dir=/yarn-logs/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.type=HEAT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.scan-interval-seconds=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.xframe.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.percentage-physical-cpu-limit=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.msgRequestSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-xattr-size=16384
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.http.address=0.0.0.0:9864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.blocks.per.postponedblocks.rescan=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.cli.prune.age=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.task.heap.mb=94208
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.web.authentication.filter=org.apache.hadoop.hdfs.web.AuthFilter
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.maintenance.replication.min=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.jetty.logs.serve.aliases=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.ugi.expire.after.access=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.op.size=52428800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.admin.acl=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reducer.unconditional-preempt.delay.sec=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.display.per-user-apps=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-removal-untracked.timeout-ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.in-memory.check-period-mins=720
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.test.drop.namenode.response.number=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.df.interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.cache.limit.max-single-resource-mb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.netty.high.watermark=65535
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.webapp.address=0.0.0.0:8089
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.balance.max.concurrent.moves=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.hostname=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.exit.timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.max-queue-length=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.token.tracking.ids.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.authorization.caching.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.failover.sleep.max.millis=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.retry.timeout.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.outgoingMessageValueClass=edu.agh.iga.adi.giraph.direction.io.data.IgaMessageWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.move.thread-count=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.permissions.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.filter.initializers=org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.allowed-container-networks=host,none,bridge
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.accept-recovery.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.client-server.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.max.retries=9
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.working.dir=/user/${user.name}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.jobname.limit=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.domain.socket.path=/var/lib/hadoop-hdfs/dn_socket
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.decommission.blocks.per.interval=500000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.write-txns.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] rpc.metrics.quantile.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.subcluster-resolver.class=org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.read-lock-reporting-threshold-ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.timeout=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.memory-mb=294912
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.failed.volumes.tolerated=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.framework.name=yarn
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.fileoutputcommitter.algorithm.version=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.clientrm.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.datanode.refresh.ip-hostname-check=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.system-metrics-publisher.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.nested-level=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.timeout=200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.generic-application-history.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.caller.context.signature.max.size=40
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.dns.log-slow-lookups.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.https.address=0.0.0.0:19890
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.table.capacity.read=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyMaxConnectionFailures=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.ping=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.masterComputeClass=edu.agh.iga.adi.giraph.direction.IterativeComputation
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.max.attempts=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.max-no-move-interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.opportunistic-containers-use-pause-for-preemption=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.cross-origin.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.control-resource-monitoring=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.oauth2.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxRequestMilliseconds=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.genericoptionsparser.used=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.servicerpc-address=iga-adi-m:8051
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.health-checker.script.timeout-ms=1200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.fileoutputcommitter.failures.attempts=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.num-retries=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.require.client.cert=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.uid.cache.secs=14400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.intermediate-data-encryption.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.minPartitionsPerComputeThread=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.speculative-cap-running-tasks=0.1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.block.id.layout.upgrade.threads=12
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.du.reserved.calculator=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.storeSolution=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.context=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.system-metrics-publisher.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.token.renew-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.app-cache-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.isStaticGraph=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.tcpnodelay=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.metrics.runtime.buckets=60,300,1440
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.intervalMsec=21600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.oob.timeout-ms=1500,0,0,0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.application-client-protocol.poll-timeout-ms=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.sharedcache.mode=disabled
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.map.index.skip=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.hdfs.file.creation.retry.wait.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.hdfs-servers=${fs.defaultFS}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.memoryObserver.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.logLevel=error
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitTaskDoneTimeoutMs=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.output.compress=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.multipart.uploads.block.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.metadata.cache.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edekcacheloader.interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.merge.progress.records=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] tfile.fs.output.buffer.size=262144
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.connection.retries=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.du.interval=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.window.num.buckets=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.retry-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.uploader.server.address=0.0.0.0:8046
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.failover.max.attempts=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.socket.send.buffer=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.locateFollowingBlock.retries=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.quorum=localhost:2181
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jvm.system-properties-to-log=os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.allowed-origins=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.enable.retrycache=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-applications=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.du.reserved=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.data.transfer.client.tcpnodelay=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.memory.mb=3072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.caller.context.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useNettyDirectMemory=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.prepare-recovery.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.transferTo.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.sensitive-config-keys=
      secret$
      password$
      ssl.keystore.pass$
      fs.s3.*[Ss]ecret.?[Kk]ey
      fs.s3a.*.server-side-encryption.key
      fs.azure.account.key.*
      credential$
      oauth.*token$
      hadoop.security.sensitive-config-keys
  
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.completion.pollinterval=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-removal-untracked.allow-empty-include=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.id=giraph_yarn_application_1584864522397_0050
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.name.dir.restore=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.full.block.report.lease.length.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.secondary.http-address=0.0.0.0:9868
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.logs.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.read.timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.initialisation.type=surface
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.token.always-use=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.system.bucket=dataproc-43b93b37-4b7e-4692-b919-138ab6d725e7-us-central1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.cache.limit.max-resources=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] END OF GIRAPH CONFIGURATION
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] Setting up container launch container for containerid=container_1584864522397_0050_01_000002
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] Conatain launch Commands :java -Xmx94208M -Xms94208M -XX:+UseNUMA -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:NewSize=9G -XX:MaxNewSize=9G -server -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -XX:OnOutOfMemoryError='free -m' -cp .:${CLASSPATH} org.apache.giraph.yarn.GiraphYarnTask 1584864522397 50 2 1 1><LOG_DIR>/task-2-stdout.log 2><LOG_DIR>/task-2-stderr.log 
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] Setting username in ContainerLaunchContext to: yarn
INFO  [YarnUtils] Adding solver-1.0-SNAPSHOT.jar to LocalResources for export.to hdfs://iga-adi-m/user/kbhit/giraph_yarn_jar_cache/application_1584864522397_0050/solver-1.0-SNAPSHOT.jar
INFO  [YarnUtils] Registered file in LocalResources :: hdfs://iga-adi-m/user/kbhit/giraph_yarn_jar_cache/application_1584864522397_0050/solver-1.0-SNAPSHOT.jar
WARN  [YarnUtils] Job jars (-yj option) didn't include giraph-core.
INFO  [YarnUtils] Registered file in LocalResources :: hdfs://iga-adi-m/user/kbhit/giraph_yarn_jar_cache/application_1584864522397_0050/giraph-conf.xml
INFO  [GiraphApplicationMaster$RMCallbackHandler] Got response from RM for container ask, allocatedCnt=1
INFO  [GiraphApplicationMaster$RMCallbackHandler] Total allocated # of container so far : 2 allocated out of 2 required.
INFO  [GiraphApplicationMaster] Launching command on a new container., containerId=container_1584864522397_0050_01_000003, containerNode=iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal:37469, containerNodeURI=iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal:8042, containerResourceMemory=113664
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] START OF GIRAPH CONFIGURATION
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobtracker.address=local
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.file-formats=TFile
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.resource.check.interval=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.posix.attr.uid.name=uidNumber
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.client.thread-count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.invalidate.limit=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
      $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
      $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
      /usr/local/share/google/dataproc/lib/*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.admin.acl=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.job.committer.cancel-timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.emit-timeline-data=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.rpc-address=0.0.0.0:8485
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxWorkers=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/system/rmstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.select-input-streams.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connection.maxidletime=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.process-kill-wait.ms=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.memoryObserver.minMsBetweenFullGcs=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.use-rpc=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.map.index.interval=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.https-address=0.0.0.0:9871
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.max-no-move-interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.multipart.uploads.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.reduces=0-2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.util.hash.type=murmur
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexInputFormatClass=edu.agh.iga.adi.giraph.direction.io.InMemoryStepInputFormat
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.min=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.path.style.access=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.min-block-size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] net.topology.script.number.args=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.initial.type=RADIAL
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.windows-container.memory-limit.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.input.fileinputformat.split.minsize=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.max.attempts=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.speculative=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.node-labels.resync-interval-ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.interval=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.maps=285
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.ubertask.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.retain-seconds=604800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.use.datanode.hostname=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.getBlocks.size=2147483648
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.am.max-attempts=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.data.dir=/hadoop/dfs/data
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.parallelcopies=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] adl.feature.ownerandgroup.enableupn=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.buffer.dir=${hadoop.tmp.dir}/s3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.finish-when-all-reducers-done=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.retry.ceiling.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.count-threshold=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.edgeInputFormatClass=edu.agh.iga.adi.giraph.direction.io.IgaEdgeInputFormat
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.metrics.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.data.dir.perm=700
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.movedWinWidth=5400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.xattrs.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.bp-ready.timeout=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.transfer.socket.send.buffer.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.ha.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client.job.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.table.capacity.write=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.recovery.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.container.log.backups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.interval-ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.cache.directives.num.responses=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.node-labels.provider.fetch-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.max.total.tasks=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.port=13562
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.maxattempts=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.resource-tracker.client.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.considerLoad=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.cross-origin.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.bind-host=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.delete.thread-count=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.pending.timeout-sec=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.mutation.acl-policy.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.proxy-user-privileges.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.speculative-cap-total-tasks=0.01
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.replication=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.slowtaskthreshold=1.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.cleaner.initial-delay-mins=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.slow.io.warning.threshold.ms=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.default-container-executor.log-dirs.permissions=710
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.skip.start.attempts=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.dns.log-slow-lookups.threshold.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.reject-unresolved-dn-topology-mapping=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size=10485760
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.admin.address=0.0.0.0:8047
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socket-timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numOutputThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.max-streams-hard-limit=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.num.users=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.mount=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.classloader=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation-enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanager.minimum.version=NONE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.size=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.compress.type=RECORD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.hdfs.configuration.version=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.done-dir=/tmp/entity-file-history/done/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-app-masters=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log.retain-seconds=10800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.local-cache.max-files-per-directory=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.retry.interval=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.new-active.rpc-timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.hostname.verifier=DEFAULT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.audit.log.token.tracking.id=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.bq.project.id=charismatic-cab-252315
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.sleep.base.millis=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.permissions.superusergroup=hadoop
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.retry.times=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socket.send.buffer.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.fileoutputcommitter.task.cleanup.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.getBlocks.min-block-size=10485760
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.initialDelay=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.edgeValueClass=edu.agh.iga.adi.giraph.direction.io.data.IgaOperationWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.maximum-allocation-mb=294912
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.io.sort.factor=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.with-user-dir=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.sleep.max.millis=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.webapp.https.address=0.0.0.0:8091
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.sleepTimeSeconds=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log.deletion-threads-count=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.rpc-timeout.ms=45000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.file.impl=org.apache.hadoop.fs.LocalFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.host=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxOpenRequestsPerWorker=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.client.user=kbhit
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.adl.oauth2.access.token.provider.type=ClientCredential
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.active-dir=/tmp/entity-file-history/active
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.cores=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.background.sleep=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-blocks-per-file=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation.key.update-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.https.address=0.0.0.0:8044
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.keytab.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lifeline.handler.ratio=0.10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.compression.codec.bzip2.library=system-native
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.skip.maxrecords=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.ping.interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.zkfc.port=8019
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.window.base=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.loadedjobs.cache.size=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.credentials.binary=/hadoop/yarn/nm-local-dir/usercache/kbhit/appcache/application_1584864522397_0050/container_1584864522397_0050_01_000001/container_tokens
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.storage.policy.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.output.filter=FAILED
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.count-logical-processors-as-cores=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.system-reserved-memory-mb=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.best-effort=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.block-pinning.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.retry-after-no-speculate=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.seqfile.local.dir=${hadoop.tmp.dir}/io/local
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ui-names=tez
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.encrypt.data.transfer.cipher.key.bitlength=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.log.level=INFO
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.sync.behind.writes=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.dispatcherThreads=200
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.application.attempt.id=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.stale.datanode.interval=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.io.sort.mb=256
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-state-store.parent-path=/rmstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.clientSendBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.client.resolve.remote.symlinks=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.min-queue-length=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.enabled.protocols=TLSv1,TLSv1.1,TLSv1.2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.cpu.vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.failover-retries=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.address=iga-adi-m:10020
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.name.dir=/hadoop/dfs/name
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.leveldb-store.compaction-interval-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.local-fs.write-limit.bytes=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.configuration.file-system-based-store=/yarn/conf
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.access.token.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD,TRACE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.retry-after-speculate=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.fileio.profiling.sampling.percentage=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.address=0.0.0.0:9866
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-aggregation.policy.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ls.limit=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.max.retries=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container.stderr.tail.bytes=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.hdfs.impl=org.apache.hadoop.hdfs.DistributedFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.timeout=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ui-on-disk-path.tez=/usr/lib/tez/tez-ui-0.9.2.war
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.reservation-system.planfollower.time-step=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.custom-header=X-XSRF-HEADER
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.automatic-failover.embedded=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.handler.count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.channelsPerServer=8
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.msgRequestWarningThreshold=1.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-container-debug-info.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.map.params=${mapreduce.task.profile.params}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.logThreadLayout=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.block-placement-policy.default.prefer-local-node=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.hosts=/etc/hadoop/conf/nodes_include
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.resource.checked.volumes.minimum=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.retry.max.attempts=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.keytab=/etc/krb5.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.cluster.max-application-priority=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation-status.time-out.ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.state-store.class=org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.max-cached-nodemanagers-proxies=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.trash.checkpoint.interval=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.http-address=0.0.0.0:8480
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nm.liveness-monitor.expiry-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useSuperstepCounters=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.merge.percent=0.66
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.retrycache.heap.percent=0.03f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.timeout=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-removal-untracked.remove-on-refresh=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.store.class=file
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max-corrupt-file-blocks-returned=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.local-dirs=/hadoop/yarn/nm-local-dir
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.block-move.timeout=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.replication=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.am.max-attempts=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.internal-timers-ttl-secs=420
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.pcores-vcores-multiplier=1.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useInputSplitLocality=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.kerberos.min.seconds.before.relogin=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.compress=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.attempt.diagnostics.limit.kc=64
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ui-web-path.tez=/tez-ui
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.steps=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.ssl=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.check.period=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.defaultFS=hdfs://iga-adi-m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max-num-blocks-to-log=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.attr.group.name=cn
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.service.handler.count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.pipeline.cache-max-size=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.sort.spill.percent=0.80
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.file-controller.TFile.class=org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.http-address=0.0.0.0:9870
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.metadatastore.impl=org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.negative-cache.secs=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.server.conf=ssl-server.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.nodemanager-client-async.thread-pool-max-size=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.yarn.nodemanager.resource.memory-mb=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.admin.address=0.0.0.0:10033
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.metadata.cache.type=FILESYSTEM_BACKED
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.startup.delay.block.deletion.sec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.health-checker.interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.keytab=/etc/krb5.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.task.container.log.backups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.heartbeat.interval=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.zookeeper.session-timeout.ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] container.java.opts=-XX:+UseNUMA -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:NewSize=9G -XX:MaxNewSize=9G -server -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -XX:OnOutOfMemoryError='free -m'
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.zkList=iga-adi-m:2181
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-aggregation.compression-type=none
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-retry-minimum-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.minimum-allowed-tasks=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.cache.revocation.timeout.ms=900000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.combine.progress.records=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.supervised=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.interceptor-class.pipeline=org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.low-latency=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.address=${yarn.nodemanager.hostname}:0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reduces=95
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.queue-placement-rules=user-group
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.expiry=43200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.session.timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] tfile.io.chunk.size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.sleep-after-disconnect.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.http-cross-origin.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.user.agent.prefix=unknown
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.content-summary.sleep-microsec=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.directoryscan.threads=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.directoryscan.interval=21600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.token.validity=36000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.graceful-fence.rpc-timeout.ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.cleaner.interval-ms=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.datanode.registration.ip-hostname-check=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.backup.http-address=0.0.0.0:50105
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.opportunistic-containers-max-queue-length=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.reservation-system.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.crypto.buffer.size=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.read.timeout=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.transfer.mode=BLOCK_TRANSFER_MODE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.ifile.readahead.bytes=4194304
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.secure=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.safemode.min.datanodes=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.wasb.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.snapshot.skip.capture.accesstime-only-change=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.http-authentication.type=simple
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.socket.read-timeout=60s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.dispatcher.drain-events.timeout=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.avoid.write.stale.datanode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.retain-seconds=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.complete.cancel.delegation.tokens=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fail-fast=${yarn.fail-fast}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multiobjectdelete.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.connection-keep-alive.timeout=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.minimum-allocation-vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.max-retries=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.retry-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.exports.allowed.hosts=* rw
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.max.threads=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.cache.size=256
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.file.buffer.size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.bind-host=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.max.block.acquire.failures=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.zkfc.nn.http.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction=0.6
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-metrics.unregister-delay-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.interval-ms.get-last-block-length=4000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.txns=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.retry.interval=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.edits.dir=/tmp/hadoop/dfs/journalnode/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-metrics.period-ms=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.connect.timeout=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.uri=file:///hadoop/yarn/system/rmstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.drain-entities.timeout.ms=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.adl.impl=org.apache.hadoop.fs.adl.Adl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.connection.timeout.ms=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.openfiles.num.responses=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.cachereport.intervalMsec=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.fd-flush-interval-secs=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.tail-edits.rolledits.timeout=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.container.log.limit.kb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resourcemanager.minimum.version=NONE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.ubertask.maxreduces=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanager-connect-retries=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.secure.mode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.retry.policy.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.idlethreshold=4000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.group.hierarchy.levels=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.logaggregation.threadpool-size-max=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.quota.by.storage.type.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.collector-service.address=${yarn.nodemanager.hostname}:8048
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.client-server.address=0.0.0.0:8045
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.connection.retries.on.timeouts=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxNumberOfUnsentRequests=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.work.multiplier.per.iteration=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.simple.anonymous.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.nodemanager-connect.retry-interval-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.times.get-last-block-length=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.app-collector.linger-period.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.authentication=simple
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.files.preserve.failedtasks=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds=259200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.streams.cache.size=256
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.finalize-segment.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.replication=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.skip.proc-count.auto-incr=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.client.thread-count=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.cache-ttl.secs=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.joblist.cache.size=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-xattrs-per-inode=32
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.work-preserving-recovery.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer.timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.wtmax=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.worker.observers=edu.agh.iga.adi.giraph.direction.performance.MemoryLogger
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.purge=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.ui-actions.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.establish.timeout=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] rpc.engine.org.apache.hadoop.yarn.api.ApplicationHistoryProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.rm.system-metrics-publisher.emit-container-events=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.invalidate.work.pct.per.iteration=0.32f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.purge.age=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.client.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.auto-update.containers=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.leveldb-store.path=${hadoop.tmp.dir}/yarn/system/confstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.maximum.data.length=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] tfile.fs.input.buffer.size=262144
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.type=simple
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.encryption.zones.num.responses=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.async.message.store.threads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.cpu.vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.decommission.interval=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-delegation-token-node.split-index=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.webhdfs.impl=org.apache.hadoop.fs.WebHdfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.pipeline.ecn=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.user.home.dir.prefix=/user
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.workaround.non.threadsafe.getpwuid=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.pmem-check-enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.inotify.max.events.per.rpc=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.maps=0-2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.ssl.file.buffer.size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.transfer.socket.recv.buffer.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.registry.class=org.apache.hadoop.registry.client.impl.FSRegistryOperationsService
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.command-opts=-Xmx2457m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.timeline-client.number-of-async-entities-to-merge=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.amlauncher.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.failover.sleep.max.millis=15000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.nm.uploader.replication.factor=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.lock.suppress.warning.interval=10s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.moverThreads=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.root=/registry
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.max.attempts=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.memoryObserver.freeMemoryFractionForGc=0.1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.fd-retain-secs=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.caller.context.max.size=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.max-age=1800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.remote-app-log-dir-suffix=logs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.restart.replica.expiration=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.dir.minimum=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.mountd.port=4242
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.merge.inmem.threshold=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.netty.low.watermark=32768
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lifeline.rpc-address=iga-adi-m:8050
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.num.checkpoints.retained=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.queuename=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.max-age-ms=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.authorization=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.aux-services.spark_shuffle.class=org.apache.spark.network.yarn.YarnShuffleService
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.client.thread-count=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.uploader.server.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.serverSendBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fslock.fair=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.split.threshold=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.scanner.volume.bytes.per.second=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.balance.bandwidthPerSec=10m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.buffer.size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.rpc-timeout.ms=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.default.chunk.view.size=32768
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.datestring.cache.size=200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.handler.count=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.acl.reservation-enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer.bandwidthPerSec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.loadedjob.tasks.max=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.clientReceiveBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.local.sas.key.mode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.threads.max=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.handler-thread-count=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.server.listen.queue.size=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.connect.max-wait.ms=900000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.detect-hardware-capabilities=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.min-replication=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.max.split.locations=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyAutoRead=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blocksize=134217728
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.is.minicluster=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.connection-keep-alive.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.threads.keepalivetime=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.shell.command.timeout=0s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-validator=basic
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.cli-check.rpc-timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.zookeeper.acl=world:anyone:rwcda
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.write.stale.datanode.ratio=0.5f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.encrypt.data.transfer=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.input.lineinputformat.linespermap=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.fetch.thread-count=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyClientThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexIdClass=org.apache.hadoop.io.IntWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.skip.checksum=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.service.shutdown.timeout=30s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.ssl.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.log.level=INFO
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.SplitMasterWorker=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation-token.max-conf-size-bytes=12800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.rm.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.quota.init-threads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.opportunistic-container-allocation.nodes-used=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.cache.limit.max-resources-mb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.metrics.logger.period.seconds=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.acl.provider.permission.pattern=^(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?)*$
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.use.datanode.hostname=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.address=0.0.0.0:0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lock.detailed-metrics.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.rpc-address=iga-adi-m:8020
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.application.name.suffix=-dataproc
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.libjars=solver-1.0-SNAPSHOT.jar
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.reformat.disabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.max-packets-in-flight=80
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.threshold=2147483647
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.memory.limit.percent=0.25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.https.server.keystore.resource=ssl-server.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.include-port-in-node-name=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] map.sort.class=org.apache.hadoop.util.QuickSort
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.dns.interface=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitForPerWorkerRequests=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.retry-delay.max.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-max-znode-size.bytes=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.progressmonitor.pollinterval=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.shuffle.log.limit.kb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.max.locked.memory=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.retrycache.expirytime.millis=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.scan.period.hours=504
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.fencing.ssh.connect-timeout=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.move.interval-ms=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.best-effort=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.disk.check.min.gap=15m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hbase-schema.prefix=prod.
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-component-length=255
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.ipc.address=0.0.0.0:9867
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.path.based.cache.retry.interval.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.misreplication.processing.limit=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.fast.upload.active.blocks=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.opportunistic-container-allocation.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.tail-edits.period=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.generic-application-history.max-applications=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.resendTimedOutRequests=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.jaas.context=Client
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.hostname=iga-adi-m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.policy.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group)
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.shell.safely.delete.limit.num.files=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0}))
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.dir=${dfs.namenode.name.dir}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.failover-retries-on-socket-timeouts=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.decommission.max.concurrent.tracked.nodes=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.server.log.slow.rpc=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.store.leveldb.path=${hadoop.tmp.dir}/mapred/history/recoverystore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.support.append=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.windows-container.cpu-limit.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.socket.reuse.keepalive=4000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.vmem-pmem-ratio=2.1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.period=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.node-labels.provider.fetch-timeout-ms=1200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.java.opts=-Xmx2457m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numInputThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hbase.coprocessor.jar.hdfs.location=/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.automatic-failover.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitTimeBetweenConnectionRetriesMs=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.webapp.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.use.dfs.network.topology=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.child.java.opts=-Xmx200m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.https.need-auth=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.write-lock-reporting-threshold-ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.block.size=32M
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.host.port=21
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.avoid.read.stale.datanode=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.retry.attempts=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.allowed-runtimes=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lease-recheck-interval-ms=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.locateFollowingBlock.initial.delay.ms=400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.cluster.acls.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.retry.max.attempts=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.aux-services=mapreduce_shuffle,spark_shuffle
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.ubertask.maxmaps=9
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.hosts.exclude=/etc/hadoop/conf/nodes_exclude
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.default-container-network=host
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-manager.thread-count=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socketcache.expiryMsec=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.app-submission.cross-platform=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.metadatastore.authoritative=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reducer.preempt.delay.sec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.path.based.cache.block.map.allocation.percent=0.25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexRequestSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.rerun-if-node-unusable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.use.ipc.callq=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.cache-store-class=org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.cache.revocation.polling.ms=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.markreset.buffer.percent=0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.lazypersist.file.scrub.interval.sec=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.retry.interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.keytab=/etc/krb5.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.project.id=charismatic-cab-252315
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.dump.dir=/tmp/.hdfs-nfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.movedWinWidth=5400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] datanode.https.port=50475
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ttl-enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.cached-dfsused.check.interval.ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.delete.debug-delay-sec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.skip.maxgroups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.trash.interval=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.mover.moverThreads=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxNumberOfSupersteps=2147483647
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource-monitor.interval-ms=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] seq.io.sort.factor=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] seq.io.sort.mb=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.instrumentation.requires.admin=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.rtmax=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.serverReceiveBufferSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.container.liveness-monitor.interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.backup.address=0.0.0.0:50100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max-lock-hold-to-release-lease-ms=25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.readahead.bytes=4194304
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.random.order=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.minWorkers=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.cleaner.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.failover-controller.graceful-fence.connection.retries=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] httpfs.buffer.size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.safemode.threshold-pct=0.999f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-monitor.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.pure.yarn.job=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.java.secure.random.algorithm=SHA1PRNG
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.dns.nameserver=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.submit.file.replication=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.fail-fast=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.cache.timeout.ms=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.file.close.num-committed-allowed=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.skip.checksum.errors=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.hostname=iga-adi-m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.load-comparator=QUEUE_LENGTH
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.acl.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.fast.upload=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.partitionClass=org.apache.giraph.partition.SimplePartition
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation-token-renewer.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.swebhdfs.impl=org.apache.hadoop.fs.SWebHdfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.application-timeouts.monitor.interval-ms=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.common.configuration.version=0.23.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.client.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.edgeRequestSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.drop.cache.behind.reads=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.outputdir=hdfs://iga-adi-m/user/kbhit/1584880683
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.use.legacy.blockreader=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.max-completed-applications=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.xframe.value=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.cleaner.period-mins=1440
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.registry.base-dir=yarnfederation/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.end-notification.max.retry.interval=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.always-scan-user-dir=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.fs-store.retry-policy-spec=2000, 500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.acl-view-job= 
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.job.task.listener.thread-count=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edit.log.autoroll.check.interval.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.resource.cpu-vcores=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.skip.proc-count.auto-incr=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.search.attr.member=member
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.client.conf=ssl-client.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.start-segment.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.root-dir=/sharedcache
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.journalnode.https-address=0.0.0.0:8481
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.background.reload=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.objects=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.max.transfer.threads=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.credential.clear-text-fallback=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.access.key.update.interval=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.memory.mb=3072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.hdfs-blocks-metadata.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.image.transfer.chunksize=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.jhist.format=json
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.https.keystore.resource=ssl-client.xml
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.connect.retry-interval.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.minimum-allocation-mb=1024
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.messageStoreFactoryClass=edu.agh.iga.adi.giraph.direction.io.InMemoryObjectMessageStoreFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.cleaner.resource-sleep-ms=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] net.topology.impl=org.apache.hadoop.net.NetworkTopology
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.seqfile.compress.blocksize=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-executor.os.sched.priority.adjustment=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.running.reduce.limit=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.hedged.read.threadpool.size=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.heartbeat.recheck-interval=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.task.overhead.percent=0.2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.safemode.extension=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.vmem-check-enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.failover.sleep.base.millis=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.key.update-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.rpc.protection=authentication
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.permissions.umask-mode=022
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useMessageSizeEncoding=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.staticuser.user=dr.who
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.maximum=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.methods-allowed=GET,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.paging.maximum=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation.token.renew-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.maximum.response.length=134217728
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.shell.missing.defaultFs.warning=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-localizer.java.opts=-Xmx256m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.replace-datanode-on-failure.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.saskey.usecontainersaskeyforallaccess=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.checkpoint.dir=file:///hadoop/dfs/namesecondary
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.rest-csrf.browser-useragents-regex=^Mozilla.*,^Opera.*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.windows.minutes=1,5,25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.use.legacy.blockreader.local=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.maxtaskfailures.per.tracker=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.max.connections=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.outEdgesClass=edu.agh.iga.adi.giraph.direction.io.IgaArrayEdges
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.application-client-protocol.poll-interval-ms=200
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.inputOutEdgesClass=edu.agh.iga.adi.giraph.direction.io.IgaArrayEdges
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.list.cache.pools.num.responses=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.server.port=2049
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.checksum.type=CRC32C
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.readahead.range=64K
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.short.circuit.replica.stale.threshold.ms=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.zookeeper.parent-znode=/hadoop-ha
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.admin.thread-count=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.cpu-vcores=96
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.http.policy=HTTP_ONLY
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.attempts.maximum=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.lazywriter.interval.sec=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.log-aggregation.retain-check-interval-seconds=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.multipart.copy.block.size=5368709120
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-ip-cache.expiry-interval-secs=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.client.fd-clean-interval-secs=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.wasbs.impl=org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.num-retries=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.split.metainfo.maxsize=10000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.random.device.file.path=/dev/urandom
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.maxRetries=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.nodemanager-connect.max-wait-ms=180000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client-am.ipc.max-retries=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-diagnostics-maximum-size=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.metadata.cache.directory=/hadoop_gcs_connector_metadata_cache
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useNettyPooledAllocator=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.replication.max=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.https.address=0.0.0.0:9865
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.standby.checkpoints=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.kill.max=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.memory=92
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.committer.setup.cleanup.needed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.domain.socket.data.traffic=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.localizer.cache.target-size-mb=10240
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.admin.client.thread-count=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hash.partitionBalanceAlgorithm=static
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.connection.timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.queue-limit-stdev=1.0f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numMasterZkInputSplitThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.zk-appid-node.split-index=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.tmp.dir=/hadoop/tmp
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.failover.sleep.base.millis=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.configuration-type=centralized
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.http.internal-proxy.port=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.ttl-ms=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.exit.timeout.check-interval-ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.speculative=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.block.size=134217728
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.recovery.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.counters.max=120
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.name.cache.threshold=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.full.block.report.leases=6
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexValueClass=edu.agh.iga.adi.giraph.direction.io.data.IgaElementWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.workerContextClass=edu.agh.iga.adi.giraph.direction.IgaWorkerContext
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.doOutputDuringComputation=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.extra.edits.segments.retained=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.webapp.ui2.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.log.level=INFO
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.file-block-storage-locations.timeout.millis=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.fuse.timer.period=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.timeout-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.check-interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.hedged.read.threshold.millis=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.retry-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.compress=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.native.lib.available=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.in-memory.staleness-period-mins=10080
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.count-limit=2048
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.providers.combined=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.running.map.limit=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.input.buffer.percent=0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.computationClass=edu.agh.iga.adi.giraph.direction.computation.initialisation.InitialComputation
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.slow.io.warning.threshold.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.multipart.size=100M
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.job.committer.commit-window=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyServerThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.new-epoch.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.working.dir=/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.asynclogging=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.bq.output.buffer.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.reader.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.incremental.intervalMsec=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.network.counts.cache.max.size=2147483647
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.minPercentResponded=100.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.writer.class=org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.ifile.readahead=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3native.replication=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.get-journal-state.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitForOtherWorkersMsec=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.summary-store=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.socketcache.capacity=16
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.table.create=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.stream-buffer-size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.retry.policy.spec=10000,6,60000,10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.socket.recv.buffer=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.fsdatasetcache.max.threads.per.volume=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.in-memory.initial-delay-mins=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.address=iga-adi-m:19888
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.adl.impl=org.apache.hadoop.fs.adl.AdlFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.userlog.limit.kb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.ssl.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.rmadmin.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.webapp.address=0.0.0.0:8788
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.fuse.connection.timeout=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.retry.policy.spec=10000,6,60000,10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.rm.container-allocation.expiry-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.server.max.connections=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.resource.mb=3072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.secs=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.peer.stats.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.shuffle.output.buffer.size=4096
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.replication=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.numComputeThreads=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.transfer.buffer.size=131072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.audit.log.async=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.directory.search.timeout=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.disk.check.timeout=10m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.checksum.combine.mode=MD5MD5CRC
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs=86400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.maximum-allocation-vcores=32000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.sleep-delay-before-sigkill.ms=250
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.acl-modify-job= 
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.automatic.close=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.sas.expiry.period=90d
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjm.operations.timeout=60s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.stale.datanode.minimum.interval=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.size=192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.background.reload.threads=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.input.fileinputformat.list-status.num-threads=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.posix.attr.gid.name=gidNumber
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.du.reserved.pct=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.acls.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.short.circuit.replica.stale.threshold.ms=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3.block.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.resource.du.reserved=104857600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.server-defaults.validity.period.ms=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.shuffle.listen.queue.size=128
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.libjars.wildcard=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.recovery.compaction-interval-secs=3600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edits.noeditlogchannelflush=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.shuffle.input.buffer.percent=0.70
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.http.policy=HTTP_ONLY
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.maxattempts=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapred.bq.gcs.bucket=dataproc-43b93b37-4b7e-4692-b919-138ab6d725e7-us-central1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.audit.loggers=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.serializations=org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.groups.cache.warn.after.ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.write.byte-array-manager.count-reset-time-period-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.snapshot.capture.openfiles=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.node-labels.fs-store.impl.class=org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.queued-edits.limit.mb=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.policy=HTTP_ONLY
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.max-size-to-move=10737418240
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.sync.behind.writes.in.background=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.acl=world:anyone:rwcda
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexOutputFormatThreadSafe=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.file-block-storage-locations.num-threads=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container.stderr.pattern={*stderr*,*STDERR*}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.cluster.local.dir=/hadoop/mapred/local
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.kerberos.kinit.command=kinit
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.secondary.https-address=0.0.0.0:9869
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.metrics.logger.period.seconds=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.access.token.lifetime=600
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.token.max-lifetime=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.drop.cache.behind.writes=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.considerLoad.factor=2.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.num.extra.edits.retained=1000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.implicit.dir.infer.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.connect.max.retries.on.timeouts=45
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.viewfs.rename.strategy=SAME_MOUNTPOINT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.client.resolve.topology.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.proxyuser.hive.hosts=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-labels.provider.fetch-interval-ms=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-metrics.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.block.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.fast.upload.buffer=disk
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ha.health-monitor.connect-retry-interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edekcacheloader.initial.delay.ms=3000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.tasktracker.map.tasks.maximum=96
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.datanode-restart.timeout=30
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.mapfile.bloom.size=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.authentication.retry-count=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.ftp.data.connection.mode=ACTIVE_LOCAL_DATA_CONNECTION_MODE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.preferIP=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore=GET,OPTIONS,HEAD
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.max.map=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.shuffle.log.backups=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.vertexOutputFormatClass=edu.agh.iga.adi.giraph.direction.io.StepVertexOutputFormat
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ftp.blocksize=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxMasterSuperstepWaitMsecs=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.kerberos.principal.pattern=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.scheduler.monitor.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.socket.connect-timeout=60s
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.replication.max-streams=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] nfs.allow.insecure.ports=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.nm.uploader.thread-count=20
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.graphPartitionerFactoryClass=edu.agh.iga.adi.giraph.direction.IgaPartitionerFactory
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.client.job.retry-interval=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.store.max-logs=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.authorization=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.version=1.0f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.am.liveness-monitor.expiry-interval-ms=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.har.impl.disable.cache=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reduce.slowstart.completedmaps=0.95
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.upgrade.domain.factor=${dfs.replication}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.minicluster.fixed.ports=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.application.classpath=$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
      $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
      /usr/local/share/google/dataproc/lib/*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delegation.token.max-lifetime=604800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.automatic-failover.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.java.opts=-Xmx2457m
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.socket.write.timeout=480000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.accesstime.precision=3600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.mapfile.bloom.error.rate=0.005
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.rest-csrf.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/timeline
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.proxyuser.hive.groups=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.support.allow.format=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.scheduler.configuration.zk-store.parent-path=/confstore
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.content-summary.limit=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.writer.flush-interval-seconds=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodes.exclude-path=/etc/hadoop/conf/nodes_exclude
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.outliers.report.interval=1800000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.shuffle.log.separate=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.user.group.static.mapping.overrides=dr.who=;
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.amrmproxy.address=0.0.0.0:8049
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.webapp.xfs-filter.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.cached.conn.retry=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.key.provider.cache.expiry=864000000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.path.based.cache.refresh.interval.ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.collector-service.thread-count=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nodes.include-path=/etc/hadoop/conf/nodes_include
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-directory-items=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.ha.log-roll.period=120
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.capabilities=CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.distributed-scheduling.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.fallback-to-simple-auth-allowed=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.fixed.ports=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.remote-app-log-dir=/yarn-logs/
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.problem.type=HEAT
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.scan-interval-seconds=60
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.xframe.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.percentage-physical-cpu-limit=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.msgRequestSize=1048576
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.fs-limits.max-xattr-size=16384
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.http.address=0.0.0.0:9864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.blocks.per.postponedblocks.rescan=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.cli.prune.age=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.yarn.task.heap.mb=94208
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.web.authentication.filter=org.apache.hadoop.hdfs.web.AuthFilter
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.maintenance.replication.min=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.jetty.logs.serve.aliases=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.ugi.expire.after.access=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.max.op.size=52428800
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.admin.acl=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.reducer.unconditional-preempt.delay.sec=300
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.app.mapreduce.am.hard-kill-timeout-ms=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.display.per-user-apps=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-removal-untracked.timeout-ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.recovery.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.store.in-memory.check-period-mins=720
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.test.drop.namenode.response.number=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.df.interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.cache.limit.max-single-resource-mb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.netty.high.watermark=65535
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.webapp.address=0.0.0.0:8089
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.balance.max.concurrent.moves=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.hostname=0.0.0.0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.exit.timeout=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.max-queue-length=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.token.tracking.ids.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.azure.authorization.caching.enable=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.failover.sleep.max.millis=2000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.mmap.retry.timeout.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.rest-csrf.custom-header=X-XSRF-Header
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms=100
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.outgoingMessageValueClass=edu.agh.iga.adi.giraph.direction.io.data.IgaMessageWritable
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.move.thread-count=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.permissions.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.filter.initializers=org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.http-authentication.simple.anonymous.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.runtime.linux.docker.allowed-container-networks=host,none,bridge
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.accept-recovery.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.client-server.thread-count=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.max.retries=9
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.working.dir=/user/${user.name}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.jobname.limit=50
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.domain.socket.path=/var/lib/hadoop-hdfs/dn_socket
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.decommission.blocks.per.interval=500000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.write-txns.timeout.ms=20000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] rpc.metrics.quantile.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.federation.subcluster-resolver.class=org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.read-lock-reporting-threshold-ms=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.timeout=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.resource.memory-mb=294912
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.failed.volumes.tolerated=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.framework.name=yarn
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.fileoutputcommitter.algorithm.version=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.router.clientrm.interceptor-class.pipeline=org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.datanode.refresh.ip-hostname-check=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.system-metrics-publisher.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.nested-level=3
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.connection.timeout=200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.generic-application-history.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.caller.context.signature.max.size=40
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.dns.log-slow-lookups.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.webapp.https.address=0.0.0.0:19890
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] file.client-write-packet-size=65536
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.s3guard.ddb.table.capacity.read=500
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.nettyMaxConnectionFailures=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.ping=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.masterComputeClass=edu.agh.iga.adi.giraph.direction.IterativeComputation
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.max.attempts=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.balancer.max-no-move-interval=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.opportunistic-containers-use-pause-for-preemption=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.webapp.cross-origin.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.minicluster.control-resource-monitoring=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.webhdfs.oauth2.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.maxRequestMilliseconds=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.genericoptionsparser.used=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.servicerpc-address=iga-adi-m:8051
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.health-checker.script.timeout-ms=1200000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.fileoutputcommitter.failures.attempts=4
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.fs.state-store.num-retries=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.ssl.require.client.cert=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.uid.cache.secs=14400
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.intermediate-data-encryption.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.minPartitionsPerComputeThread=1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.speculative.speculative-cap-running-tasks=0.1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.block.id.layout.upgrade.threads=12
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.du.reserved.calculator=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.storeSolution=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.context=default
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.system-metrics-publisher.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.token.renew-interval=86400000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.timeline-service.entity-group-fs-store.app-cache-size=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.isStaticGraph=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] ipc.client.tcpnodelay=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.metrics.runtime.buckets=60,300,1440
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.blockreport.intervalMsec=21600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.oob.timeout-ms=1500,0,0,0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.client.application-client-protocol.poll-timeout-ms=-1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.sharedcache.mode=disabled
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] io.map.index.skip=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.hdfs.file.creation.retry.wait.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.hdfs-servers=${fs.defaultFS}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.memoryObserver.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.logLevel=error
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.waitTaskDoneTimeoutMs=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.map.output.compress=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3n.multipart.uploads.block.size=67108864
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.metadata.cache.enable=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.edekcacheloader.interval.ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.merge.progress.records=10000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] tfile.fs.output.buffer.size=262144
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.failover.connection.retries=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.du.interval=600000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.top.window.num.buckets=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.zk.retry-interval-ms=1000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.sharedcache.uploader.server.address=0.0.0.0:8046
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.http.client.failover.max.attempts=15
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.s3a.socket.send.buffer=8192
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.client.block.write.locateFollowingBlock.retries=5
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.zk.quorum=localhost:2181
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.jvm.system-properties-to-log=os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.cross-origin.allowed-origins=*
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.enable.retrycache=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-applications=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.du.reserved=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.data.transfer.client.tcpnodelay=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.xfs-filter.xframe-options=SAMEORIGIN
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.reduce.memory.mb=3072
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.caller.context.enabled=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] giraph.useNettyDirectMemory=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.qjournal.prepare-recovery.timeout.ms=120000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.datanode.transferTo.allowed=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.sensitive-config-keys=
      secret$
      password$
      ssl.keystore.pass$
      fs.s3.*[Ss]ecret.?[Kk]ey
      fs.s3a.*.server-side-encryption.key
      fs.azure.account.key.*
      credential$
      oauth.*token$
      hadoop.security.sensitive-config-keys
  
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.client.completion.pollinterval=5000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.node-removal-untracked.allow-empty-include=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.id=giraph_yarn_application_1584864522397_0050
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.name.dir.restore=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.full.block.report.lease.length.ms=300000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.secondary.http-address=0.0.0.0:9868
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.http.logs.enabled=true
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] hadoop.security.group.mapping.ldap.read.timeout.ms=60000
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] iga.initialisation.type=surface
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] s3.bytes-per-checksum=512
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] dfs.namenode.delegation.token.always-use=false
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] fs.gs.system.bucket=dataproc-43b93b37-4b7e-4692-b919-138ab6d725e7-us-central1
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] mapreduce.job.cache.limit.max-resources=0
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] END OF GIRAPH CONFIGURATION
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] Setting up container launch container for containerid=container_1584864522397_0050_01_000003
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] Conatain launch Commands :java -Xmx94208M -Xms94208M -XX:+UseNUMA -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:NewSize=9G -XX:MaxNewSize=9G -server -XX:+UnlockDiagnosticVMOptions -XX:+PrintFlagsFinal -XX:OnOutOfMemoryError='free -m' -cp .:${CLASSPATH} org.apache.giraph.yarn.GiraphYarnTask 1584864522397 50 3 1 1><LOG_DIR>/task-3-stdout.log 2><LOG_DIR>/task-3-stderr.log 
INFO  [GiraphApplicationMaster$LaunchContainerRunnable] Setting username in ContainerLaunchContext to: yarn
INFO  [GiraphApplicationMaster$RMCallbackHandler] Got response from RM for container ask, completedCnt=1
INFO  [GiraphApplicationMaster$RMCallbackHandler] Got container status for containerID=container_1584864522397_0050_01_000003, state=COMPLETE, exitStatus=0, diagnostics=
INFO  [GiraphApplicationMaster$RMCallbackHandler] After completion of some containers. Waiting on others. Current status is: completedCount :1 containersToLaunch :2 successfulCount :1 failedCount :0
INFO  [GiraphApplicationMaster$RMCallbackHandler] Got response from RM for container ask, completedCnt=1
INFO  [GiraphApplicationMaster$RMCallbackHandler] Got container status for containerID=container_1584864522397_0050_01_000002, state=COMPLETE, exitStatus=0, diagnostics=
INFO  [GiraphApplicationMaster$RMCallbackHandler] All container compeleted. done = true
INFO  [GiraphApplicationMaster] Done true
INFO  [GiraphApplicationMaster] Forcefully terminating executors with done =:true
INFO  [GiraphApplicationMaster] Application completed. Stopping running containers
INFO  [GiraphApplicationMaster] Application completed. Signalling finish to RM
INFO  [GiraphApplicationMaster] Giraph Application Master completed successfully. exiting

End of LogType:gam-stdout.log
*******************************************************************************


End of LogType:prelaunch.err
******************************************************************************

Container: container_1584864522397_0050_01_000001 on iga-adi-m.us-central1-a.c.charismatic-cab-252315.internal_37469
LogAggregationType: AGGREGATED
====================================================================================================================
LogType:prelaunch.out
LogLastModifiedTime:Sun Mar 22 12:38:33 +0000 2020
LogLength:70
LogContents:
Setting up env variables
Setting up job resources
Launching container

End of LogType:prelaunch.out
******************************************************************************

